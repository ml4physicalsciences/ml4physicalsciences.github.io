<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-48900508-9"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-48900508-9');
		</script>

		<title>Machine Learning and the Physical Sciences, NeurIPS 2021</title>
		<meta name="description" content="Website for the Machine Learning and the Physical Sciences (MLPS) workshop at the 35th Conference on Neural Information Processing Systems (NeurIPS)">
		<meta name="author" content="Atilim Gunes Baydin">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<link rel="stylesheet" href="assets/css/lightbox.css" />
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<img style="width:5em;"src="images/NeurIPS-logo-white.svg" alt="" />
						<h1>Machine Learning and the Physical Sciences</h1>
						<p>Workshop at the 35th Conference on Neural Information Processing Systems (NeurIPS)<br />
						December 13 or 14, 2021</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#about" class="active">About</a></li>
							<!-- <li><a href="#survey" class="active"><font color="red"><sup>NEW!</sup></font>Survey</a></li> -->
							<!-- <li><a href="#papers">Accepted papers</a></li> -->
					    <li><a href="#schedule">Schedule</a></li>
							<!-- <li><a href="#papers">Papers/Posters</a></li> -->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#cfp">Call for papers</a></li> 
							<!-- <li><a href="#registration">Registration</a></li> -->
							<!-- <li><a href="#sponsors">Sponsors</a></li> -->
							<!-- <li><a href="#location">Location</a></li> -->
							<li>
								<div class="dropdown">
								  <a>All Years</a>
								  <div class="dropdown-content">
										<a href="https://ml4physicalsciences.github.io/2021">2021</a>
										<a href="https://ml4physicalsciences.github.io/2020">2020</a>
										<a href="https://ml4physicalsciences.github.io/2019">2019</a>
								    <a href="https://ml4physicalsciences.github.io/2017">2017</a>
								  </div>
								</div>
							</li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<section id="about" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>About</h2>
									</header>
									<p>Machine learning methods have had great success in learning complex representations of data that enable novel modeling and data processing approaches in many scientific disciplines. Physical sciences span problems and challenges at all scales in the universe: from finding exoplanets in trillions of sky pixels, to developing solutions to the quantum many-body problem and combinatorial problems, to detecting anomalies in event streams from the Large Hadron Collider, to predicting how extreme weather events will vary with climate change. Tackling a number of associated data-intensive tasks including, but not limited to, segmentation, computer vision, sequence modeling, causal reasoning, generative modeling, and probabilistic inference are critical for furthering scientific discovery in these and many other areas. In addition to using machine learning models for scientific discovery, the ability to interpret what a model has learned is receiving an increasing amount of attention.</p>

									<p>In this targeted workshop, we aim to bring together computer scientists, mathematicians and physical scientists who are interested in applying machine learning to various outstanding physical problems including in inverse problems, approximating physical processes, understanding what a learned model represents, and connecting tools and insights from the physical sciences to the study of machine learning models. In particular, the workshop invites researchers to contribute short papers (extended abstracts) that demonstrate cutting-edge progress in the application of machine learning techniques to real-world problems in the physical sciences and/or using physical insights to understand and improve machine learning techniques.</p>

									<p>By bringing together machine learning researchers and physical scientists who apply machine learning, we expect to strengthen the interdisciplinary dialogue, introduce exciting new open problems to the broader community, and stimulate the production of new approaches to solving challenging open problems in the sciences. Invited talks from leading individuals in both communities will cover the state-of-the-art techniques and set the stage for this workshop.</p>

									<h2>NeurIPS 2021</h2>
									<p><span class="image left"><img style="width:6em;padding-left:0.5em;padding-top:0.3em;" src="images/NeurIPS-logo.svg" alt="" /></span>The Machine Learning and the Physical Sciences 2021 workshop will be held on December 13 or 14, 2021 as a part of the <a href="https://neurips.cc/">35th Annual Conference on Neural Information Processing Systems</a>. Originally planned to be at the Vancouver Convention Centre, Vancouver, BC, Canada, NeurIPS 2021 and this workshop will take place entirely virtually (online). Please check the <a href="https://neurips.cc/">main conference website</a> for the latest information.</p>
								</div>
							</div>
						</section>

							<section id="schedule" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Schedule</h2>
										</header>
									</div>
								</div>

								TO BE ANNOUNCED</br></br></br>

								<!-- <h2>How to attend</h2>
								<p>The workshop is taking place on Friday, December 11, 2020. It will be a combination of: (1) streamed invited talks and Q&A sessions, which can be watched live via SlidesLive or Zoom, and (2) poster sessions for accepted papers and community development breakouts, which are hosted in Gather Town.</p>

								<p><b>Streamed talks and Q&A sessions:</b> follow the <a href="https://neurips.cc/virtual/2020/protected/workshop_16129.html">Official NeurIPS ML4PS schedule</a> link on top of this page, which will take you to the page with the live SlidesLive stream and a RocketChat box where you can ask questions during the Q&A sessions. On the same page you can also find a <a href="https://zoom.us/download">Zoom</a> link to join the same stream as a Zoom webinar (optional).</p>

								<p><b>Poster sessions and community development breakouts:</b> follow the <a href="https://neurips.gather.town/app/GS7AwXNphTXVVEZH/NeurIPS%20ML4PS">ML4PS Gather Town</a> link on top of this page that will take you to the workshop's <a href="https://gather.town/">Gather Town</a> space, an interactive online environment where you can walk around and browse the posters, and interact with poster presenters and other attendees. This is the same setup used by NeurIPS main conference, and you can see an introductory video about Gather Town poster sessions <a href="https://slideslive.com/38946995/poster-sessions-in-gathertown">here</a>.</p>

								<p>Note that all accepted papers, posters and poster videos are also accessible publicly on this web page below.</p>
								</br> -->

								<!-- <h3>Check the times for your local time zone</h3>

								<ul>
									<li>First go to <a href="https://neurips.cc/virtual/2020/protected/cal_main.html">NeurIPS general schedule</a> page. You can use the time zone selection dropdown menu to set your preference.</li>
									<li>Afterwards when you go to the <a href="https://neurips.cc/virtual/2020/protected/workshop_16129.html">workshop's schedule</a> page, the times should appear in your selected time zone.</li>
								</ul>

								<p>Note: the times given below are in US/Eastern (UTC-5).</p>

								<h3>Session 1</h3>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td width="20%">10:00 AM – 10:10 AM</td>
												<td width="15%">SlidesLive/Zoom</td>
												<td>
													Opening remarks (live)<br>
												</td>
											</tr>
											<tr>
												<td>10:10 AM – 10:35 AM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Invited talk: "3D Milky Way Dust Map using a Scalable Gaussian Process" (live)<br>
													<b><a href="https://obs.carnegiescience.edu/users/landerson">Lauren Anderson</a></b> (Carnegie Observatories)<br>
												</td>
											</tr>
											<tr>
												<td>10:35 AM – 10:45 AM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Q&A with Lauren Anderson (live)<br>
												</td>
											</tr>
											<tr>
												<td>10:45 AM – 11:10 AM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Invited talk: "Geometric Deep Learning for Functional Protein Design" (live)<br>
													<b><a href="https://www.imperial.ac.uk/people/m.bronstein">Michael Bronstein</a></b> (Imperial College London)<br>
												</td>
											</tr>
											<tr>
												<td>11:10 AM – 11:20 AM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Q&A with Michael Bronstein (live)<br>
												</td>
											</tr>
											<tr>
												<td>11:20 AM – 12:50 PM</td>
												<td>Gather Town</td>
												<td>
													Poster session (live)<br>
												</td>
											</tr>
										</tbody>
									</table>
								</div>

								<h3>Session 2</h3>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td width="20%">12:50 PM – 12:55 PM</td>
												<td width="15%">SlidesLive/Zoom</td>
												<td>
													Opening remarks (live)<br>
												</td>
											</tr>
											<tr>
												<td>12:55 PM – 01:20 PM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Invited talk: "Variational Neural Annealing"<br>
													<b><a href="https://www.perimeterinstitute.ca/people/estelle-maeva-inack">Estelle Inack</a></b> (Perimeter Institute)<br>
												</td>
											</tr>
											<tr>
												<td>01:20 PM – 01:30 PM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Q&A with Estelle Inack (live)<br>
												</td>
											</tr>
											<tr>
												<td>01:30 PM – 01:55 PM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Invited talk: "Generative Flow Models for Gauge Field Theory"<br>
													<b><a href="https://web.mit.edu/physics/people/faculty/shanahan_phiala.html">Phiala Shanahan</a></b> (Massachusetts Institute of Technology)<br>
												</td>
											</tr>
											<tr>
												<td>01:55 PM – 02:05 PM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Q&A with Phiala Shanahan (live)<br>
												</td>
											</tr>
											<tr>
												<td>02:05 PM – 03:35 PM</td>
												<td>Gather Town</td>
												<td>
													Poster session (live)<br>
												</td>
											</tr>
										</tbody>
									</table>
								</div>

								<h3>Session 3</h3>
								<div class="table-wrapper">
									<table>
										<tbody>
											<tr>
												<td width="20%">03:35 PM – 03:40 PM</td>
												<td width="15%">SlidesLive/Zoom</td>
												<td>
													Opening remarks (live)<br>
												</td>
											</tr>
											<tr>
												<td>03:40 PM – 04:05 PM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Invited talk: "Physics-based Learning for Computational Microscopy"<br>
													<b><a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/waller.html">Laura Waller</a></b> (UC Berkeley)<br>
												</td>
											</tr>
											<tr>
												<td>04:05 PM – 04:15 PM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Q&A with Laura Waller (live)<br>
												</td>
											</tr>
											<tr>
												<td>04:15 PM – 05:45 PM</td>
												<td>Gather Town</td>
												<td>
													Community development breakouts (live)<br>
												</td>
											</tr>
											<tr>
												<td>05:45 PM – 06:15 PM</td>
												<td>SlidesLive/Zoom</td>
												<td>
													Feedback from community development breakouts (live)<br>
												</td>
											</tr>
										</tbody>
									</table>
								</div>
							</section>
 -->


 							<section id="organizers" class="main">
 								<div class="spotlight">
 									<div class="content">
 										<header class="major">
 											<h2>Organizers</h2>
 										</header>
 										<ul class="features">
 											<li>
 												<img style="width:10em;" src="images/baydin.jpg" alt=""/>
 												<h3><a href="http://www.robots.ox.ac.uk/~gunes/">Atılım Güneş Baydin</a></br>University of Oxford</h3>
 											</li>
 											<li>
 												<img style="width:10em;" src="images/carrasquilla.jpg" alt=""/>
 												<h3><a href="https://vectorinstitute.ai/team/juan-felipe-carrasquilla/">Juan Felipe Carrasquilla</a></br>Vector Institute / University of Waterloo</h3>
 											</li>
 											<li>
 												<img style="width:10em;" src="images/kucukbenli.jpg" alt=""/>
 												<h3><a href="https://www.bu.edu/questrom/profile/emine-kucukbenli/">Emine Kucukbenli</a></br>Harvard University / Boston University</h3>
 											</li>
											<li>
 												<img style="width:10em;" src="images/louppe.jpg" alt=""/>
 												<h3><a href="https://glouppe.github.io/">Gilles Louppe</a></br>University of Liège</h3>
 											</li>
 										</ul>
 										<ul class="features">
 											<li>
 												<img style="width:10em;" src="images/nachman.jpg" alt=""/>
 												<h3><a href="https://bids.berkeley.edu/people/benjamin-nachman">Benjamin Nachman</a></br>Lawrence Berkeley National Laboratory</h3>
 											</li>
 											<li>
 												<img style="width:10em;" src="images/nord.jpg" alt=""/>
 												<h3><a href="https://computing.fnal.gov/brian-nord/">Brian Nord</a></br>Fermilab</h3>
 											</li>
 											<li>
 												<img style="width:10em;" src="images/thais.jpg" alt=""/>
 												<h3><a href="https://www.linkedin.com/in/savannah-thais-12a7b95a/">Savannah Thais</a></br>Princeton University / IRIS-HEP</h3>
 											</li>
 										</ul>
 										<header class="major">
 											<h2>Steering Committee</h2>
 										</header>
 										<ul class="features">
 											<li>
 												<img style="width:10em;" src="images/anandkumar.jpg" alt=""/>
 												<h3><a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a></br>Caltech / NVIDIA</h3>
 											</li>
 											<li>
 												<img style="width:10em;" src="images/cranmer.jpg" alt=""/>
 												<h3><a href="http://theoryandpractice.org/">Kyle Cranmer</a></br>New York University</h3>
 											</li>
 											<!-- <li>
 												<img style="width:10em;" src="images/ho.png" alt=""/>
 												<h3><a href="https://users.flatironinstitute.org/~sho/index.html">Shirley Ho</a></br>Flatiron / Princeton / Carnegie Mellon</h3>
 											</li> -->
 											<li>
 												<img style="width:10em;" src="images/prabhat.jpg" alt=""/>
 												<h3><a href="http://www.nersc.gov/about/nersc-staff/data-analytics-services/prabhat/">Prabhat</a></br>NERSC, Berkeley Lab</h3>
 											</li>
 											<li>
 												<img style="width:10em;" src="images/speaker_zdeborova.jpg" alt=""/>
 												<h3><a href="http://artax.karlin.mff.cuni.cz/~zdebl9am/">Lenka Zdeborova</a></br>Institut de Physique Théorique</h3>
 											</li>
 										</ul>
 									</div>
 								</div>
 							</section>


														<section id="cfp" class="main">
															<div class="spotlight">
																<div class="content">
																	<header class="major">
																		<h2>Call for papers</h2>
																	</header>

																	<p>In this targeted workshop, we aim to bring together physical scientists and machine learning researchers who are interested in applying machine learning to various outstanding physical problems; or are using physical insights to understand and improve machine learning techniques.</p>

<p>We invite researchers to submit work particularly in the following and related areas:</p>
<ul>
	<li>High-impact applications of machine learning to physical sciences, experiment or theory</li>
	<li>Strategies for incorporating prior scientific knowledge into machine learning algorithms</li>
	<li>Application of physical sciences to understand, model and improve machine learning techniques</li>
	<li>Machine learning model interpretability for obtaining insights to physical systems</li>
	<li>Any other area related to the subject of the workshop, including but not limited to techniques that are relevant to physical systems such as variational inference, simulation-based inference, probabilistic models and so on.</li>
</ul>

<p>Submissions of completed projects as well as high-quality works in progress are welcome. Submissions will be kept confidential until they are accepted and authors confirm that they can be included in the workshop. If a submission is not accepted, or withdrawn for any reason, it will be kept confidential and not made public.</p> 

<p>Accepted work will be presented as posters during the workshop. Please note that at least one coauthor of each accepted paper will be expected to have a NeurIPS conference registration that includes the workshop session and participate in one of the virtual poster sessions.</p>

<h3>Important note for work that will be/has been published elsewhere:</h3>

<p>All accepted short papers (extended abstracts) will be made available on the workshop website. This does not constitute an archival publication or formal proceedings; authors retain full copyright of their work and are free to publish their extended work in another journal or conference. We allow submission of extended abstracts that overlap with papers that are under review or have been recently published in a conference or a journal, including physical science journals. However, we do not accept cross submissions of the same content to multiple workshops at NeurIPS. </p>
	


																	<!--
																	<p>We invite researchers to submit work particularly in the following and related areas:</p>
																	<ul>
																		<li>Application of machine learning to physical sciences</li>
																		<li>Generative models</li>
																		<li>Likelihood-free inference</li>
																		<li>Variational inference</li>
																		<li>Simulation-based inference</li>
																		<li>Implicit models</li>
																		<li>Probabilistic models</li>
																		<li>Model interpretability</li>
																		<li>Approximate Bayesian computation</li>
																		<li>Strategies for incorporating prior scientific knowledge into machine learning algorithms</li>
																		<li>Experimental design</li>
																		<li>Any other area related to the subject of the workshop</li>
																	</ul>
																	<p>Submissions of completed projects as well as high-quality works in progress are welcome. All accepted short papers (extended abstracts) will be made available on the workshop website. This does not constitute an archival publication or formal proceedings; authors retain full copyright of their work and are free to publish their extended work in another journal or conference. We allow submission of extended abstracts that overlap with papers that are under review or have been recently published in a conference or a journal. However, we do not accept cross submissions of the same extended abstract to multiple workshops at NeurIPS. Submissions will be kept confidential until they are accepted and authors confirm that they can be included in the workshop. If a submission is not accepted, or withdrawn for any reason, it will be kept confidential and not made public.</p>

																	<p>Accepted work will be presented as posters during the workshop. Please note that at least one coauthor of each accepted paper will be expected to have a NeurIPS conference registration that includes the workshop session and participate in one of the virtual poster sessions.</p>
																-->

																	<h3>Submission instructions</h3>

																	<p>Submissions should be anonymized short papers (extended abstracts) up to 4 pages in PDF format, typeset using the <a href="https://neurips.cc/Conferences/2021/PaperInformation/StyleFiles">NeurIPS style</a>. The authors are required to include a short statement (one paragraph) about the potential broader impact of their work, including any ethical aspects and future societal consequences, which may be positive or negative. The broader impact statement should come after the main paper content (see <a href="https://neurips.cc/Conferences/2021/PaperInformation/StyleFiles">the NeurIPS style files</a> for an example). The impact statement and references do not count towards the page limit. Appendices are discouraged, and reviewers are not expected to read beyond the first 4 pages and the impact statement. A workshop-specific modified NeurIPS style file will be provided for the camera-ready versions, after the author notification date.</p>

																	<p>The submission link will be shared here on this page in the coming weeks.</p>
																	<!-- <p>Submissions page is <a href="https://cmt3.research.microsoft.com/ML4PS2020">here</a>. </p> -->

																	<!-- <a href="https://cmt3.research.microsoft.com/ML4PS2020" class="button">Submit paper</a> -->
																	<p></p>

																	<!-- <h2>Instructions for Accepted Papers</h2>

																	<h3>Camera-ready papers</h3>
																	<p>Please produce a "camera-ready" (final) version of your accepted paper by replacing the "neurips_2020.sty" style file with the "neurips_2020_ml4ps.sty" file <a href="files/neurips_2020_ml4ps.sty">available here</a> and using the "final" package option (that is, "\usepackage[final]{neurips_2020_ml4ps}") to include author and affiliation information. The modified style file replaces the first page footer to correctly refer to the workshop instead of the main conference. It is acceptable if your paper goes up to five pages (excluding the broader impact statement, acknowledgments, references, and any appendices) due to author and affiliation information taking extra space on the first page. The five-page limit is strict, and appendices are allowed but discouraged.</p>

																	<p>Please revise your paper as much as you can to reasonably address reviewer comments. The revision would include minor corrections and/or changes to directly address reviewer comments. Beyond these points, it is not acceptable to include any significant new material that was not present in the reviewed version of your paper.</p>

																	<p>Please upload the final PDF of your paper by the camera-ready deadline, by logging in to the <a href="https://cmt3.research.microsoft.com/ML4PS2020">submission website</a> and using the camera-ready link shown with your submission.</p>

																	<h3>Posters and optional videos</h3>

																	<p>The poster sessions will take place virtually in several <a href="https://gather.town/">GatherTown</a> sessions. Posters will be presented during live and interactive sessions with virtual poster boards, whereby the presenter and the participants will interact with audio and video. You will have control of your audio and video and can turn them on and off at any point as you wish. GatherTown emulates a physical poster session venue where attendees can freely walk from poster to poster and interact in groups with the presenters through audio/video. Posters will be visible in full within the GatherTown platform and the attendees will have the possibility to zoom in to parts of your poster.</p>

																	<p>Optionally, you can produce a 5-minute video in addition to your poster, upload it to YouTube and provide the YouTube URL for us to share in GatherTown. The video would be a brief presentation of your work described in the paper and the poster.</p>

																	<p>Posters and optional videos will also be shared on the website of the workshop.</p>

																	<ul>
																	<li>Please prepare an A0 landscape poster PDF. The landscape orientation ensures that your poster is seen best in computer screens. We have observed a regular A0 format works well in the GatherTown interface.</li>
																	<li>Submit your poster together with your optional video using the Google form <a href="https://forms.gle/ioDqznEWfNbUbX1i6">here</a></li>
																	<li>The poster deadline is set to December 4, 2020, 23:59 PDT.</li>
																	</ul> -->

																	<h3>Important dates</h3>
																	<ul>
																		<li>Submission deadline: September 18, 2021, 23:59 <a href="https://www.timeanddate.com/time/zones/aoe">AoE</a></li>
																		<li>Author notification: October 15, 2021</li>
																		<li>Camera-ready (final) paper deadline: November 15, 2021, 23:59 <a href="https://www.timeanddate.com/time/zones/aoe">AoE</a></li>
																		<!-- <li>Poster deadline: December 4, 2020, 23:59 <a href="https://www.timeanddate.com/time/zones/aoe">AoE</a></li> -->
																		<li>Workshop: December 13 or 14, 2021</li>
																	</ul>

																	<h3 id="registration">Registration</h3>
																	<p>NeurIPS conference has three main sessions (Tutorials, Conference, Workshops) to which you can register. You need to be registered to at least the Workshop session in order to be able to attend this workshop. For the latest registration-related information please refer to <a href="https://neurips.cc">NeurIPS 2021 website</a>.
																</div>
															</div>
														</section>

									<!-- <section id="sponsors" class="main">
										<div class="spotlight">
											<div class="content">
												<header class="major">
													<h2>Sponsors</h2>
												</header>
												<ul class="features">
													<li style="margin: 0em 8em 2em 0em;">
														<h3><a style="text-decoration: none;color: #FFFFFF;" href="https://vectorinstitute.ai/"><img style="width:200%;"src="images/vector_institute_logo.png" alt="Vector Institute" /></a></h3>
													</li>
												</ul>
											</div>
										</div>
									</section> -->

								<!-- <h2>Program Committee (Reviewers)</h2>
								<p>We acknowledge the program committee for providing insightful reviews on a very tight schedule (in alphabetical order):</p>
								<p>Aaron So, Abigail Azari, Adi Hanuka, Aditi Krishnapriyan, Ahmed Mazari, Alireza Sheikhattar, Amit Kumar Jaiswal, Ana Belen Espinosa Gonzalez, Andrea Marchini, Andreas K Maier, Andrzej Banburski, Aneesh Rangnekar, Anindita Maiti, Anoop Kulkarni, Antoine Wehenkel, Aranildo Lima, Arash Broumand, Arijit Patra, Arrykrishna Mootoovaloo, Artem Maevskiy, Arun Baskaran, Arya Farahi, Ashish Mahabal, Ashwin Balakrishna, Auralee Edelen, Behrooz Mansouri, Ben Albrecht, Benjamin Nachman, Bishnu Sarker, Bradley Gram-Hansen, Budhaditya Deb, Chase Shimmin, Christoph Feinauer, Christoph Weniger, Christopher Tunnell, Cleber Zanchettin, Cora Dvorkin, Cory Stephenson, Craig Jones, Cristiano De Nobili, Daniel Bedau, Daniel W. Fonteles Alves, Daniel E Worrall, David Pfau, David Rousseau, Devansh Agarwal, Dhagash Mehta, Dimitrios Korkinof, Donini Julien, Elif Ozkirimli, Elijah Cole, Enrico Rinaldi, Erick Moen, Erwan Allys, Evan Shellshear, Fabian Ruehle, Filippo Vicentini, Francisco Villaescusa-Navarro, Frank Noe, Frank Soboczenski, Frederic A Dreyer, George Williams, Gilles Louppe, Gilles Orban de Xivry, Giovanni Turra, Grant Rotskoff, Guillaume Mahler, Hao Wu, Haoran Liu, Haoxiang Wang, Harkirat Singh Behl, Hasan Poonawala, Himaghna Bhattacharjee, Hossein Sharifi Noghabi, Jaan Altosaar, Jaehoon Lee, Jake Searcy, Janardan Misra, Jason X. Dou, Jason Poulos, Javier Duarte, Jean-Roch Vlimant, Jennifer Wei, Jesse Thaler, Jessica Forde, Jesús E. Ortíz, Jize Zhang, Joakim Andén, Joeri Hermans, Johann Brehmer, Johanna Hansen, John Arevalo, Jordan Hoffmann, Joyjit Kundu, Juan Carrasquilla, Kadri B. Ozutemiz, Kazuhiro Terao, Keegan Stoner, Kees Benkendorfer, Keiran Thompson, Kevin Yang, Kim Nicoli, Lu Lu, Luca Saglietti, Lucas Vinh Tran, Maghesree Chakraborty, Marcel Schmittfull, Mariel N Pettee, Mario Krenn, Markus Stoye, Matteo Manica, Matthew Beach, Matthew Schwartz, Matthia Sabatelli, Matthias Degroote, Maurizio Pierini, Melanie Weber, Michael Albergo, Michael Kagan, Michelle Ntampaka, Mike Williams, Miles Cranmer, Mohamed Hibat-Allah, Mohammad M Sultan, Murtaza Safdari, Mustafa Mustafa, Naeemullah Khan, Nalini Kumar, Nathanael Assefa, Neofytos Dimitriou, Niranjan Sridhar, Nishan Srishankar, Nkosinathi Ndlovu, Octavi Obiols-Sales, Olivier Absil, Olmo Cerri, Omar Jamil, Ouail Kitouni, Pablo de Castro Manzano, Pablo Martin, Patrick Kominske, Patrick McCormack, Peer-Timo Bremer, Peetak Mitra, Peter M Melchior, Peter Sadowski, Prabhakar Marepalli, Pradyumna Singh, Prakash Mishra, Praneet Dutta, Praveen T N, Rachel Kurchin, Rachneet Kaur, Rajanie Prabha, Richard Feder, Rob Zinkov, Robert A Barton, Roberto Bondesan, Robin Sandkuehler, Rodrigo A. Vargas Hernández, Rogan Carr, Rushil Anirudh, Sadanand Singh, Samuel S Schoenholz, Samuel Yen-Chi Chen, Samujjwal Ghosh, Sandhya Prabhakaran, Sarah Marzen, Sascha Diefenbacher, Satpreet H Singh, Sean Paradiso, Sebastian Goldt, Sethu Sankaran, Sheng Liu, Shivang Shekhar, Siddha Ganju, Siddharth Jain, Siddharth Mishra Sharma, Simon Olsson, Simon Stieber, Sivaramakrishnan Swaminathan, Srikant Veeraraghavan, Stefano Carrazza, Stephan Hoyer, Steven Atkinson, Steven Farrell, Sucheta Jawalkar, Sujay S. Kumar, Sven Krippendorf, Syed M. Ali, Tal Kachman, Tan Minh Nguyen, Tatiana Likhomanenko, Thomas Adler, Thong Nguyen, Tiffany J Vlaar, Tilman Plehn, Tommaso Dorigo, Tomo Lazovich, Tsuyoshi Okita, Tzu-Chi Yen, Valentina Salvatelli, Venkat Viswanathan, Vladimir Milian, Waad Subber, Wahid Bhimji, Wanli Wu, William Shipman, Xiangyang Ju, Yangzesheng Sun, Yann Coadou, Yuefeng Zhang, Yves Mabiala, Zeeshan Ahmad, Zelong Zhang, Zengyi Li, Zhe Liu, Zhonghua Zheng</p> -->
							</section>



							<!-- <section id="sponsors" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Sponsors</h2>
										</header>
										<h2>TO COME</h2>
									</div>
								</div>
							</section> -->




							<section id="location" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Location</h2>
										</header>
										<p>Originally planned to be at the <a href="https://www.vancouverconventioncentre.com/">Vancouver Convention Centre</a>, Vancouver, BC, Canada, NeurIPS 2021 and this workshop will take place entirely virtually (online). Please check the <a href="https://neurips.cc/">main conference website</a> for the latest information.</p>
										<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d20818.955675001147!2d-123.13290035008346!3d49.28834401330276!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x5486719d2eb7670d%3A0xe0b2e92e436b5515!2s1055+Canada+Pl%2C+Vancouver%2C+BC+V6C+0C3%2C+Canada!5e0!3m2!1sen!2suk!4v1566481208277!5m2!1sen!2suk" width="100%" height="300em" frameborder="0" style="border:0" allowfullscreen></iframe>
									</div>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>Contact</h2>
							<p>For questions and comments, please contact: <a href="mailto:gunes@robots.ox.ac.uk">gunes@robots.ox.ac.uk</a></p>
						</section>
						<p class="copyright">Background image: <a href="https://www.spacetelescope.org/images/potw1712a/">NGC 3447 from Hubble WFC3</a></p>
						<p class="copyright">Copyright &copy; Atılım Güneş Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</br>Design inspired by <a href="http://bayesiandeeplearning.org/">http://bayesiandeeplearning.org/</a> by Yarin Gal.</p>
					</footer>

			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
			<!-- Default Statcounter code for
			ml4physicalsciences.github.io
			https://ml4physicalsciences.github.io/ -->
			<script type="text/javascript">
			var sc_project=12052169;
			var sc_invisible=1;
			var sc_security="0155c36a";
			var sc_https=1;
			</script>
			<script type="text/javascript"
			src="https://www.statcounter.com/counter/counter.js"
			async></script>
			<noscript><div class="statcounter"><a title="free hit
			counter" href="https://statcounter.com/"
			target="_blank"><img class="statcounter"
			src="https://c.statcounter.com/12052169/0/0155c36a/1/"
			alt="free hit counter"></a></div></noscript>
			<!-- End of Statcounter Code -->
			<script src="assets/js/lightbox.js"></script>
	</body>
</html>
